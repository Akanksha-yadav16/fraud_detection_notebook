# Fraud Detection Case Study - Jupyter Notebook
# =============================================

# ðŸ“Œ Step 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from xgboost import XGBClassifier

# Ignore warnings
import warnings
warnings.filterwarnings("ignore")

# ==========================================================
# ðŸ“Œ Step 2: Load Dataset
# (Change file path as needed. Use nrows for faster testing)
# ==========================================================
df = pd.read_csv("Fraud.csv")  # load subset for speed
df.head()

# ==========================================================
# ðŸ“Œ Step 3: Explore Dataset
# ==========================================================
print(df.shape)
print(df.info())
print(df.describe())

# Check missing values
print(df.isnull().sum())

# ==========================================================
# ðŸ“Œ Step 4: Data Cleaning
# ==========================================================
# Fill missing values (example: numeric â†’ median, categorical â†’ mode)
for col in df.columns:
    if df[col].dtype in ["int64", "float64"]:
        df[col] = df[col].fillna(df[col].median())
    else:
        df[col] = df[col].fillna(df[col].mode()[0])

# ðŸš€ Skipped outlier removal (models like XGBoost/RandomForest are robust enough)

# Drop highly correlated features (multicollinearity check)
corr_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(10,6))
sns.heatmap(corr_matrix, cmap="coolwarm", annot=False)
plt.show()

# Example: Drop features with correlation > 0.9
threshold = 0.9
cols_to_drop = [column for column in corr_matrix.columns if any(corr_matrix[column].abs() > threshold) and column != "isFlaggedFraud"]
df.drop(columns=cols_to_drop, inplace=True, errors="ignore")

# Drop ID-like columns (too many unique values, not useful)
df.drop(columns=["nameOrig", "nameDest"], inplace=True, errors="ignore")

# Encode only 'type' (transaction type)
df = pd.get_dummies(df, columns=["type"], drop_first=True)


# ==========================================================
# ðŸ“Œ Step 5: Split Data
# ==========================================================
X = df.drop("isFlaggedFraud", axis=1)
y = df["isFlaggedFraud"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Scale features for Logistic Regression
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ==========================================================
# ðŸ“Œ Step 6: Build Models
# ==========================================================

# Logistic Regression
log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train_scaled, y_train)
y_pred_log = log_model.predict(X_test_scaled)

print("=== Logistic Regression Report ===")
print(classification_report(y_test, y_pred_log))
print("AUC:", roc_auc_score(y_test, log_model.predict_proba(X_test_scaled)[:,1]))

# XGBoost Model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric="logloss")
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

print("=== XGBoost Report ===")
print(classification_report(y_test, y_pred_xgb))
print("AUC:", roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:,1]))

# ==========================================================
# ðŸ“Œ Step 7: Feature Importance
# ==========================================================
importances = xgb_model.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10,6))
sns.barplot(x=importances[indices], y=X.columns[indices])
plt.title("Feature Importance - XGBoost")
plt.show()

# ==========================================================
# ðŸ“Œ Step 8: Model Evaluation Plots
# ==========================================================
fpr, tpr, _ = roc_curve(y_test, xgb_model.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label="XGBoost (AUC = %0.2f)" % roc_auc_score(y_test, y_pred_xgb))
plt.plot([0,1],[0,1],'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()
